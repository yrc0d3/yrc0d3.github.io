# [CSAPP]第五章 优化程序性能


编写高效程序需要做到以下几点：
1. 我们必须选择一组适当的算法和数据结构。
2. 我们必须编写出编译器能够有效优化以转换成高效可执行代码的源代码。
3. 针对处理运算量特别大的计算，将一个任务分解成多个部分，这些部分可以在多核和多处理器的某种组合上并行地计算。（第12章再讲）

通常，程序员必须在实现和维护程序的简单性与它的运行速度之间做出权衡。

为了使程序性能最大化，程序员和编译器都需要一个目标机器的模型，指明如何处理指令，以及各个操作的时序特性。程序员必须理解这些处理器是如何工作的，从而调整他们的程序以获得最大的速度。基于Intel和AMD处理器最近的设计，我们提出了这种机器的一个高级模型。我们还设计了一种图形数据流(data-flow)表示法，可以是处理器对指令的执行形象化，我们还可以利用它预测程序的性能。

了解了处理器的运作，我们就可以进行程序优化的第二步，利用处理器提供的指令级并行(instruction-level parallelism)能力，同时执行多条指令。

研究程序的汇编代码表示是理解编译器以及产生的代码会如何运作的最有效手段之一。

## 5.1 优化编译器的能力和局限性

现代编译器运用复杂精细的算法来确定一个程序中计算的是什么值，以及它们是被如何使用的。然后会利用一些机会来简化表达式，在几个不同的地方使用同一个计算，以及降低一个给定的计算必须被执行的次数。大多数编译器，包括GCC，向用户提供了一些对它们所使用的优化的控制，其中最简单的控制就是指定**优化级别**。

编译器必须很小心地对程序只使用**安全的**优化，也就是说对于程序可能遇到的所有可能的情况，在C语言标准提供的保证之下，优化后得到的程序和未优化的版本有一样的行为。

下面看个示例：

```
void twiddle1(long *xp, long *yp)
{
    *xp += *yp;
    *yp += *xp;
}

void twiddle2(long *xp, long *yp)
{
    *xp += 2* *yp;
}
```

乍一看，两个过程似乎有相同的行为。但当xp等于yp时，结果就不同了。这种两个指针可能指向同一个内存位置的情况称为**内存别名使用(memory aliasing)**。在只执行安全的优化中，编译器必须假设不同的指针可能会指向内存中同一个位置。

第二个妨碍优化的因素是函数调用。当被优化的函数有**副作用**时，很可能优化后的行为是错误的。

包含函数调用的代码可以用一个称为内联函数替换(inline substitution或inlining)的过程进行优化。此时，将函数调用替换为函数体。在某些情况下，最好能阻止编译器执行内联替换，如GDB。

## 5.2 表示程序性能

我们引入度量标准**每元素的周期数(Cycles Per Element, CPE)**，作为一种表示程序新能并指导我们改进代码的方法。CPE这种度量标准帮助我们在更细节的级别上理解迭代程序的循环性能。这样的度量标准对执行重复计算的程序来说是很适当的。

处理器活动的顺序是由时钟控制的，时钟提供了某个频率的规律信号，通常用**千兆赫兹(GHz)**，即十亿周期每秒来表示。每个时钟周期的时间是时钟频率的倒数。从程序员的角度来看，用时钟周期来表示度量标准要比用纳秒或者皮秒来表示有帮助得多。用时钟周期来表示，度量值表示的是执行了多少条指令，而不是时钟运行得有多快。

图5-1中的函数用来计算向量的前置和(prefix sum)。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_1.png)

这样一个过程所需要的时间可以用一个常数加上一个与被处理元素个数成正比的因子来描述。图5-2是这两个函数需要的周期数关于n的取值范围图。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_2.png)

使用最小二乘拟合(least squares fit)，psum1和psum2的运行时间分别接近等式`368+9.0n`和`368+6.0n`。其中n的系数称为**每元素的周期数(CPE)**的有效值。

## 5.3 程序示例

```
typedef struct {
    long len;
    data_t *data;
} vec_rec, *vec_prt;
```

`data_t`表示基本元素的数据类型。在测试中，我们度量代码对于整数(C语言的int和long)和浮点数(C语言的float和double)数据的性能，因此会使用类似如下的声明：

```
typedef long data_t;
```

图5-4给出的是一些生成向量、访问向量元素以及确定向量长度的基本过程。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_4.png)

作为一个优化示例，图5-5中的代码，使用某种运算，讲一个向量中所有的元素合并成一个值。通过使用编译时常数IDENT和OP的不同定义，这段代码可以重编译成对数据执行不同的运算。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_5.png)

以下声明可对向量的元素求和：
 
```
#define IDENT 0
#define OP +
```

以下声明可对向量的元素求乘积：
 
```
#define IDENT 1
#define OP *
```
 
在我们的讲述中，我们会对这段代码进行一系列的变化，写出这个合并函数的不同版本。
 
作为一个起点，下表给出的是combine1的CPE度量值，它运行在我们的参考机上(一台Intel Core i7 Haswell处理器机器)。实验显示，32位整数和64位整数，性能相同。浮点数也一样。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_5_result.png)

从中可以看出，使用命令行选项"-O1"进行基本优化，都能显著提高程序性能。
 
## 5.4 消除循环的低效率

识别循环中要执行多次但是计算结果不会改变的计算，将其移动到循环外面。

图5-6是combine2的代码。与combine1相比，性能提升了不少。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_6.png)

## 5.5 减少过程调用

图5-9是combine3的代码。与combine2相比，内循环中没有函数调用。但性能与combine2相比基本没有提升。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_9.png)

## 5.6 消除不必要的内存引用

图5-10是combine4的代码。与combine3相比，每次迭代的内存操作，从两次读和一次写，减少到只需要一次读。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_10.png)

## 5.7 理解现代处理器

如果试图进一步提高性能，必须考虑利用处理器微体系结构的优化，也就是处理器用来执行指令的底层系统设计。

在代码级上，看上去似乎是一次执行一条指令，每条指令都包括从寄存器或内存取值，执行一个操作，并把结果存回到一个寄存器或内存位置。在实际的处理器中，是同时对多条指令求值的，这个现象称为**指令级并行**。

虽然现代微处理器的详细设计超出了本书讲授的范围，对这些微处理器的原则有一般性的了解就足够理解它们如何实现指令级并行。我们会发现两种下界描述了程序的最大性能。当一系列操作必须按照严格顺序执行时，就会遇到**延迟界限(latency bound)**，因为在下一条指令之前，这条指令必须结束。当代码中的数据相关限制了处理器利用指令级并行的能力时，延迟界限能够限制程序性能。**吞吐量界限(throughput bound)**刻画了处理器功能单元的原始计算能力。这个界限时程序性能的终极限制。

### 5.7.1 整体操作

图5-11是现代微处理器的一个非常简单化的示意图。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_11.png)

这些处理器在每个时钟周期可以执行多个操作，而且是乱序的。整个设计中有两个主要部分：**指令控制单元(Instruction Control Unit, ICU)**和**执行单元(Execution Unit, EU)**。前者负责从内存中读出指令序列，并根据这些指令序列生成一组针对程序数据的基本操作；而后者执行这些操作。

ICU从**指令高速缓存(instruction cache，包含最近访问的指令)**中读取指令。通常ICU会在当前正在执行的指令很早之前取指，这样它才有足够的时间对指令译码，并把操作发送到EU。**指令控制(fetch control)**包括分支预测，以完成确定取哪些指令的任务。

**指令译码(instruction decode)**接收实际的程序指令，并把它们转换成一组基本操作。这种译码逻辑对指令进行分解，允许任务在一组专门的硬件单元之间进行分割。这些单元可以并行地执行多条指令的不同部分。

EU接收来自取指单元的操作。通常，每个时钟周期会接收多个操作。这些操作会被分派到一组**功能单元(functional units)**中，他们会执行实际的操作。这些功能单元用来处理不同类型的操作。

在ICU中，**退役单元(retirement unit)**记录正在进行的处理，并确保它遵守机器级程序的顺序语义。任何对程序寄存器的更新都只会在指令退役时才会发生，只有在处理器能够确信导致这条指令的所有分支都预测正确了，才会这样做。为了加速一条指令到另一条指令的结果的传送，许多此类信息时在执行单元之间交换的，即图中的**操作结果(opration results)**。

### 5.7.2 功能单元的性能

图5-12是Intel Core i7 Haswell参考机的一些算术运算的性能：

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_12.png)

表示运算性能的指标：
- 延迟(latency)：表示完成运算所需要的总时间
- 发射时间(issue time)：表示两个连续的同类型的运算之间需要的最小时钟周期数
- 容量(capacity)：表示能够执行该运算的功能单元的数量

可以看到，加法和乘法的发射时间都是1，意思是每个时钟周期，处理器都可以开始一条新的这样的运算。这种很短的发射时间是通过使用流水线(pipeline)实现的。

### 5.7.3 处理器操作的抽象模型

这一小节介绍了一个分析在现代处理器上执行的机器级程序性能的工具--程序的**数据流(data-flow)**表示。这是一种图形化的表示方法，展现了不同操作之间的数据相关是如何限制它们的执行顺序的。这些限制形成了图中的**关键路径(critical path)**，这是执行一组机器指令所需时钟周期数的一个下界。

#### 1. 从机器级代码到数据流图

程序的数据流表示是非正式的。我们只是想用它来形象地描述程序中的数据相关是如何主宰程序的性能的。下面以combine4(图5-10)为例来描述数据流表示法。我们将注意力集中在循环执行的计算上，因为对于大向量来说，这是决定性能的主要因素。

我们考虑类型为double的数据、以乘法作为合并运算的情况。这个循环编译出的代码如下：

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_7_3_1_instructions.png)

如图5-13，在我们假想的处理器设计中，指令译码器会把这4条指令扩展成为一系列的五步操作：

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_13.png)

顶部方框表示循环开始时寄存器的值，底部的方框表示最后寄存器的值。

对于形成循环的代码片段，我们可以将访问到的寄存器分成四类：
- 只读：%rax
- 只写：本例中没有
- 局部：这些寄存器在循环内部被修改和使用，迭代与迭代之间不相关。本例中的条件码寄存器就是。
- 循环：对于循环来说，这些寄存器既作为源值，又作为目的，一次迭代中产生的值会在另一次迭代中使用。%rds和%xmm0就是。

正如我们会看到的，循环寄存器之间的操作链决定了限制性能的数据相关。

图5-14是对图5-13的进一步改进，目标是只给出影响程序执行时间的操作和数据相关。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_14.png)

图5-15给出了函数combine4内循环的n次迭代的数据流表示。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_15.png)

我们可以看到，程序又两条数据相关链，分别对应操作mul和add对程序值acc和data+i的修改。假设浮点乘法延迟为5个周期，整数加法延迟为1个周期，那么左边的链会成为关键路径。

#### 2. 其他性能因素

对于整数加法的情况，我们对combine4的测试表明CPE为1.27，而不是根据图5-15中预测的CPE为1，测试值比预测值要慢。这说明，数据流表示中的关键路径提供的只是程序需要周期数的下界。还有其他一些因素会限制性能，包括可用的功能单元的数量和任何一步中功能单元之间能够传递数据值的数量。

总结一下combine4的性能分析：我们对程序操作的抽象数据流表示说明，combine4的关键路径长`L * n`是由对程序值acc的连续更新造成的，这条路径将CPE限制为最多L。

看上去，延迟界限是基本的限制，决定了我们的合并运算能够执行多快。接下来的任务是重新调整操作的结构，增强指令的并行性。我们想对程序做变换，使得唯一的限制变成吞吐量界限，得到接近于1.00的CPE。

## 5.8 循环展开

循环展开是一种程序变换，通过增加每次迭计算的元素的数量，减少循环的迭代次数。

图5-16是合并代码使用“$2 \times 1$循环展开”的版本。每次迭代，循环索引i加2，在一次迭代中对数据元素i和i+1使用合并运算。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_16.png)

按这个思想归纳为对一个循环按任意因子k进行展开，由此产生$k \times 1$循环展开，它可以将性能改进到达到延迟界限，但不能超过。

## 5.9 提高并行性

## 5.9.1 多个累积变量 

图5-21展示了“$2 \times 2$循环展开”。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_21.png)

归纳可得到$k \times k$循环展开。当k足够大时，程序性能可达到吞吐量界限。

## 5.9.2 重新结合变换

图5-26给出了combine7，它与combine5的区别在于内循环中元素合并的方式。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_26.png)

```
// combine5
acc = (acc OP data[i]) OP data[i+1];
// combine7
acc = acc OP (data[i] OP data[i+1]);
```

差别仅在于两个括号时如何放置的。我们称之为**重新结合变换(reassociation transformatino)**。

整数加法的性能没变化，整数乘法、浮点加法、浮点乘法的性能，则提升了一倍。

总的来说，重新结合变换能够减少计算中关键路径上操作的数量，通过更好地利用功能单元的流水线能力得到更好的性能。

## 5.10 优化合并代码的结果小结

下表总结了对于标量(scalar)代码所获得的结果。使用多项优化技术，我们获得的CPE已经接近于0.50和1.00的吞吐量界限，只受限于功能单元的容量。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_10_result.png)

## 5.11 一些限制因素

### 5.11.1 寄存器溢出

循环并行性的好处受汇编代码描述计算的能力限制。如果我们的并行度p超过了可用的寄存器数量，那么编译器会诉诸**溢出(spilling)**，将某些临时值存放到内存中，通常是在运行时堆栈上分配空间。

一旦编译器必须要诉诸寄存器溢出，那么维护多个累积变量的优势就很可能消失。幸运的是，x86-64有足够多的寄存器，大多数循环出现寄存器溢出之前就将达到吞吐量限制。

### 5.11.2 分支预测和预测错误处罚

#### 1. 不要过分关心可预测的分支
#### 2. 书写适合用条件传送实现的代码

## 5.12 理解内存性能

到目前为止，我们写的所有代码和测试，只访问相对较少的内存。

所有的现代处理器都包含一个或多个**高速缓存(cache)**存储器，以对这样少量的存储器提供快速的访问。本节会进一步研究涉及加载(从内存读到寄存器)和存储(从寄存器写到内存)操作的程序的性能，只考虑所有的数据都存放在高速缓存中的情况。

### 5.12.1 加载的性能

一个包含加载操作的程序的性能既依赖于流水线的能力，也依赖于加载单元的延迟。

要确定一台机器上加载造作的延迟，我们可以建立由一系列加载操作组成的一个计算，一条加载操作的结果决定下一条操作的地址。图5-31中计算链表长度的函数就是例子。测试表明函数list_len的CPE为4.00，我们认为这直接表明了加载操作的延迟。事实上，这个测试结果与文档中参考机的L1级cache的4周期访问时间是一致的，相关内容将在6.4节中讨论。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_31.png)

### 5.12.2 存储的性能

存储操作的性能，尤其是与加载操作的相互关系，包括一些很细微的问题。

与记载操作一样，在大多数情况下，存储操作能够在完全流水线化的模式中工作，每个周期开始一条新的存储。存储操作并不影响任何的寄存器值，因此一系列存储操作不会产生数据相关。只有加载操作会受存储操作结果的影响，因为只有加载操作能从由存储操作写的那个位置读回值。

图5-33说明了加载和存储操作之间可能的相互影响。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_33.png)

经过测试，示例A的CPE1.3，示例B的CPE为7.3。示例B中存在写/读相关(write/read dependency)--一个内存读的结果依赖于一个最近的内存写，导致处理速度下降约6个时钟周期。

为了了解原因，我们必须更加仔细地看看加载和存储执行单元，如图5-34所示。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_34.png)

存储单元包含一个**存储缓冲区(store buffer)**，它包含已经被发射到存储单元而又还没完成的存储操作的地址和数据，这里的完成包括更新cache。提供这样一个缓冲区，使得一系列存储操作不必等待每个操作都更新cache就能够执行。当一个加载操作完成时，它必须检查存储缓冲区中的条目，看有没有地址相匹配。如果有地址相匹配，它就取出相应的数据条目作为加载操作的结果。

图5-35给出了这个循环代码的数据流表示。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_35.png)

图5-36说明了write_read内循环操作之间的数据相关。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_36.png)

图5-37说明的是内循环的多次迭代形成的数据相关。

![](https://raw.githubusercontent.com/yrc0d3/imagehosting/master/img/csapp_5_37.png)

对于示例A，有不同的源和目的地址，加载和存储操作可以独立进行，因此唯一的关键路径是由减少变量cnt形成的，这使得CPE等于1.0。对于示例B，源和目的地址相同，s_data和load指令之间的数据相关使得关键路径的形成包括了存储、加载、增加数据。顺序执行这三个操作一共需要7个时钟周期。

这两个例子说明，内存操作的实现包括许多细微之处。对于寄存器操作，在指令被译码成操作的时候，处理器就可以确定哪些指令会影响其他哪些指令。另一方面，对于内存操作，只有到计算出加载和存储的地址被计算出来以后，处理器才能确定哪些指令会影响其他的哪些。高效地处理内存操作对许多程序的性能来说至关重要。内存子系统使用了很多优化，例如当操作可以独立地进行时，就利用这种潜在的并行性。

## 5.13 应用：性能提高技术

优化程序性能的基本策略：
1. 高级设计。合适的算法和数据结构
2. 基本编码原则。
    1. 消除连续的函数调用
    2. 消除不必要的内存引用
3. 低级优化。结构化代码以利用硬件功能。
    1. 展开循环，降低开销
    2. 找到方法提高指令级并行
    3. 用功能性的风格重写条件操作，使得编译采用条件传送。

## 5.14 确认和消除性能瓶颈

本节描述如何使用**代码剖析程序(code prifiler)**，优化大型程序。

### 5.14.1 程序剖析

程序剖析(profiling)运行程序的一个版本，其中插入了工具代码，以确定程序的各个部分需要多少时间。

Unix系统提供了一个剖析程序GPROF。这个程序产生两种形式的信息。首先，它确定程序中每个函数花费了多少CPU时间。其次，它计算每个函数被调用的次数，以执行调用的函数来分类。

GPROF使用方法略。

### 5.14.2 使用剖析程序来知道优化

略。
